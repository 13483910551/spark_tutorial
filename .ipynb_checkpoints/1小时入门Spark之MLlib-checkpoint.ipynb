{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T06:53:16.533008Z",
     "start_time": "2019-09-30T06:53:16.355Z"
    }
   },
   "source": [
    "## 1小时入门Spark之MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib是Spark的机器学习库，包括以下主要功能。\n",
    "\n",
    "* 实用工具：线性代数，统计，数据处理等工具\n",
    "* 特征工程：特征提取，特征转换，特征选择\n",
    "* 常用算法：分类，回归，聚类，协同过滤，降维\n",
    "* 模型优化：模型评估，参数优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib库包括两个不同的部分\n",
    "\n",
    "spark.mllib 包含基于rdd的机器学习算法API，目前不再更新，在3.0版本后将会丢弃，不建议使用。\n",
    "\n",
    "spark.ml 包含基于DataFrame的机器学习算法API，可以用来构建机器学习工作流Pipeline，推荐使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，MLlib基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame: MLlib中数据的存储形式，其列可以存储特征向量，标签，以及原始的文本，图像。\n",
    "\n",
    "Transformer：转换器。具有transform方法。通过附加一个或多个列将一个DataFrame转换成另外一个DataFrame。\n",
    "\n",
    "Estimator：估计器。具有fit方法。它接受一个DataFrame数据作为输入后经过训练，产生一个转换器Transformer。\n",
    "\n",
    "Pipeline：流水线。具有setStages方法。顺序将多个Transformer和1个Estimator串联起来，得到一个流水线模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，Pipeline流水线范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务描述：用逻辑回归模型预测句子中是否包括”spark“这个单词。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:25:38.654258Z",
     "start_time": "2019-09-30T09:25:38.566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:26: error: object ml is not a member of package org.apache.spark\n",
       "       import org.apache.spark.ml.feature._\n",
       "                               ^\n",
       "<console>:27: error: object ml is not a member of package org.apache.spark\n",
       "       import org.apache.spark.ml.classification.LogisticRegression\n",
       "                               ^\n",
       "<console>:28: error: object ml is not a member of package org.apache.spark\n",
       "       import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator,BinaryClassificationEvaluator}\n",
       "                               ^\n",
       "<console>:29: error: object ml is not a member of package org.apache.spark\n",
       "       import org.apache.spark.ml.{Pipeline,PipelineModel}\n",
       "                               ^\n",
       "<console>:30: error: object ml is not a member of package org.apache.spark\n",
       "       import org.apache.spark.ml.linalg.Vector\n",
       "                               ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator,BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T08:47:11.985069Z",
     "start_time": "2019-09-30T08:47:11.736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@3cb7bce\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@3cb7bce"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "   .master(\"local[4]\").appName(\"ml\")\n",
    "   .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，准备数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:15:04.899205Z",
     "start_time": "2019-09-30T09:15:04.513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----+\n",
      "| id|             text|label|\n",
      "+---+-----------------+-----+\n",
      "|  0|  a b c d e spark|  1.0|\n",
      "|  1|            a c f|  0.0|\n",
      "|  2|spark hello world|  1.0|\n",
      "|  3| hadoop mapreduce|  0.0|\n",
      "|  4|     I love spark|  1.0|\n",
      "|  5|         big data|  0.0|\n",
      "+---+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dftrain = [id: bigint, text: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, text: string ... 1 more field]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dftrain = Seq((0L,\"a b c d e spark\",1.0),\n",
    "                (1L,\"a c f\",0.0),\n",
    "                (2L,\"spark hello world\",1.0),\n",
    "                (3L,\"hadoop mapreduce\",0.0),\n",
    "                (4L,\"I love spark\", 1.0),\n",
    "                (5L,\"big data\",0.0)).toDF(\"id\",\"text\",\"label\")\n",
    "dftrain.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，构建模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:05:48.275051Z",
     "start_time": "2019-09-30T11:05:47.425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.ml.feature.Tokenizer\n",
      "class org.apache.spark.ml.feature.HashingTF\n",
      "class org.apache.spark.ml.classification.LogisticRegression\n",
      "class org.apache.spark.ml.Pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_a186006a3ca3\n",
       "hashingTF = hashingTF_89fe3cde38ef\n",
       "lr = logreg_8d6a68ed16b5\n",
       "pipe = pipeline_2a8d7c734272\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_2a8d7c734272"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "println(tokenizer.getClass)\n",
    "\n",
    "val hashingTF = new HashingTF().setNumFeatures(100)\n",
    "   .setInputCol(tokenizer.getOutputCol)\n",
    "   .setOutputCol(\"features\")\n",
    "println(hashingTF.getClass)\n",
    "\n",
    "val lr = new LogisticRegression().setLabelCol(\"label\")\n",
    "//println(lr.explainParams)\n",
    "lr.setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.01)\n",
    "println(lr.getClass)\n",
    "\n",
    "val pipe = new Pipeline().setStages(Array(tokenizer,hashingTF,lr))\n",
    "println(pipe.getClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3，训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:21:10.812706Z",
     "start_time": "2019-09-30T09:21:10.123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.ml.PipelineModel"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model = pipeline_8c6ec9126745\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_8c6ec9126745"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipe.fit(dftrain)\n",
    "print(model.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:08:05.789848Z",
     "start_time": "2019-09-30T09:08:05.703Z"
    }
   },
   "source": [
    "**4，使用模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:27:35.556496Z",
     "start_time": "2019-09-30T09:27:34.887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  7|           spark job|  1.0|\n",
      "|  9|         hello world|  0.0|\n",
      "| 10|           a b c d e|  0.0|\n",
      "| 11|      you can you up|  0.0|\n",
      "| 12|spark is easy to ...|  1.0|\n",
      "+---+--------------------+-----+\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|                text|            features|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|           spark job|(100,[5,70],[1.0,...|[0.35046042897667...|       1.0|\n",
      "|         hello world|(100,[48,50],[1.0...|[0.33560921515515...|       1.0|\n",
      "|           a b c d e|(100,[22,61,70,78...|[0.19082246657270...|       1.0|\n",
      "|      you can you up|(100,[25,28,33],[...|[0.81519423235142...|       0.0|\n",
      "|spark is easy to ...|(100,[5,21,60,81,...|[0.47768327161195...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dftest = [id: bigint, text: string ... 1 more field]\n",
       "dfresult = [id: bigint, text: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, text: string ... 6 more fields]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dftest = Seq((7L,\"spark job\",1.0),(9L,\"hello world\",0.0),\n",
    "                 (10L,\"a b c d e\",0.0),(11L,\"you can you up\",0.0),\n",
    "                (12L,\"spark is easy to use.\",1.0)).toDF(\"id\",\"text\",\"label\")\n",
    "dftest.show\n",
    "\n",
    "val dfresult = model.transform(dftest)\n",
    "\n",
    "dfresult.selectExpr(\"text\",\"features\",\"probability\",\"prediction\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:08:10.813260Z",
     "start_time": "2019-09-30T09:08:10.532Z"
    }
   },
   "source": [
    "**5，评估模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:36:42.815369Z",
     "start_time": "2019-09-30T09:36:42.376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name (default: label, current: label)\n",
      "metricName: metric name in evaluation (f1|weightedPrecision|weightedRecall|accuracy) (default: f1, current: accuracy)\n",
      "predictionCol: prediction column name (default: prediction, current: prediction)\n",
      "\n",
      "accuracy = 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_853d92ed6dd6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_853d92ed6dd6"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "    .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "\n",
    "println(evaluator.explainParams())\n",
    "\n",
    "println(s\"\\naccuracy = ${evaluator.evaluate(dfresult)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6，保存模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:11:36.792176Z",
     "start_time": "2019-09-30T11:11:36.206Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"mymodel.model\")\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "//model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// We can also save this unfit pipeline to disk\n",
    "//pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:52:05.433590Z",
     "start_time": "2019-09-30T09:52:04.533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|           spark job|  1.0|       1.0|\n",
      "|         hello world|  0.0|       1.0|\n",
      "|           a b c d e|  0.0|       1.0|\n",
      "|      you can you up|  0.0|       0.0|\n",
      "|spark is easy to ...|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_loaded = pipeline_8c6ec9126745\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_8c6ec9126745"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//重新载入模型\n",
    "val model_loaded = PipelineModel.load(\"pipe.model\")\n",
    "model_loaded.transform(dftest).select(\"text\",\"label\",\"prediction\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用spark.read导入csv，image，libsvm，txt等格式数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:52:35.459487Z",
     "start_time": "2019-09-30T11:52:35.114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfimage = [image: struct<origin: string, height: int ... 4 more fields>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[image: struct<origin: string, height: int ... 4 more fields>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//导入图片\n",
    "\n",
    "val dfimage = spark.read.format(\"image\").option(\"dropInvalid\", true).load(\"../imagedata\")\n",
    "dfimage.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:52:46.987458Z",
     "start_time": "2019-09-30T11:52:40.230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "|              origin|height|width|nChannels|mode|                data|\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "|file:///Users/lia...|   803|  998|        4|  24|[F2 F9 FC FF F2 F...|\n",
      "|file:///Users/lia...|   626|  886|        4|  24|[FF FF FF FF FF F...|\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfimage.selectExpr(\"image.*\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark的特征处理功能主要在 spark.ml.feature 模块中，包括以下一些功能。\n",
    "\n",
    "* 特征提取：Tf-idf, Word2Vec, CountVectorizer, FeatureHasher\n",
    "\n",
    "* 特征转换：OneHotEncoderEstimator, Normalizer, Imputer(缺失值填充), StandardScaler, MinMaxScaler, Tokenizer(构建词典), \n",
    "  StopWordsRemover, SQLTransformer, Bucketizer, Interaction(交叉项), Binarizer(二值化), n-gram,……\n",
    "\n",
    "* 特征选择：VectorSlicer(向量切片), RFormula, ChiSqSelector(卡方检验)\n",
    "\n",
    "* LSH转换：局部敏感哈希广泛用于海量数据中求最邻近，聚类等算法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 七，聚类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 八，降维模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 九，模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 十，统计工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
