{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T06:53:16.533008Z",
     "start_time": "2019-09-30T06:53:16.355Z"
    }
   },
   "source": [
    "## 1小时入门Spark之MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib是Spark的机器学习库，包括以下主要功能。\n",
    "\n",
    "* 实用工具：线性代数，统计，数据处理等工具\n",
    "* 特征工程：特征提取，特征转换，特征选择\n",
    "* 常用算法：分类，回归，聚类，协同过滤，降维\n",
    "* 模型优化：模型评估，参数优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib库包括两个不同的部分\n",
    "\n",
    "spark.mllib 包含基于rdd的机器学习算法API，目前不再更新，在3.0版本后将会丢弃，不建议使用。\n",
    "\n",
    "spark.ml 包含基于DataFrame的机器学习算法API，可以用来构建机器学习工作流Pipeline，推荐使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.apache.spark:spark_mllib_2.11:2.3.1 for download\n",
      "-> Failed to resolve org.apache.spark:spark_mllib_2.11:2.3.1\n",
      "    -> not found: /var/folders/bs/pw656yts35qb1dpr1myl3wd80000gn/T/toree-tmp-dir1813761629305680892/toree_add_deps/cache/org.apache.spark/spark_mllib_2.11/ivy-2.3.1.xml\n",
      "    -> not found: https://repo1.maven.org/maven2/org/apache/spark/spark_mllib_2.11/2.3.1/spark_mllib_2.11-2.3.1.pom\n",
      "Obtained 0 files\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.apache.spark spark_mllib_2.11 2.3.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，MLlib基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame: MLlib中数据的存储形式，其列可以存储特征向量，标签，以及原始的文本，图像。\n",
    "\n",
    "Transformer：转换器。具有transform方法。通过附加一个或多个列将一个DataFrame转换成另外一个DataFrame。\n",
    "\n",
    "Estimator：估计器。具有fit方法。它接受一个DataFrame数据作为输入后经过训练，产生一个转换器Transformer。\n",
    "\n",
    "Pipeline：流水线。具有setStages方法。顺序将多个Transformer和1个Estimator串联起来，得到一个流水线模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，Pipeline流水线范例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务描述：用逻辑回归模型预测句子中是否包括”spark“这个单词。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:25:38.654258Z",
     "start_time": "2019-09-30T09:25:38.566Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator,BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.{Pipeline,PipelineModel}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T08:47:11.985069Z",
     "start_time": "2019-09-30T08:47:11.736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@f4ea567\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@f4ea567"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "   .master(\"local[4]\").appName(\"ml\")\n",
    "   .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，准备数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:15:04.899205Z",
     "start_time": "2019-09-30T09:15:04.513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----+\n",
      "| id|             text|label|\n",
      "+---+-----------------+-----+\n",
      "|  0|  a b c d e spark|  1.0|\n",
      "|  1|            a c f|  0.0|\n",
      "|  2|spark hello world|  1.0|\n",
      "|  3| hadoop mapreduce|  0.0|\n",
      "|  4|     I love spark|  1.0|\n",
      "|  5|         big data|  0.0|\n",
      "+---+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dftrain = [id: bigint, text: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, text: string ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dftrain = Seq((0L,\"a b c d e spark\",1.0),\n",
    "                (1L,\"a c f\",0.0),\n",
    "                (2L,\"spark hello world\",1.0),\n",
    "                (3L,\"hadoop mapreduce\",0.0),\n",
    "                (4L,\"I love spark\", 1.0),\n",
    "                (5L,\"big data\",0.0)).toDF(\"id\",\"text\",\"label\")\n",
    "dftrain.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，构建模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:05:48.275051Z",
     "start_time": "2019-09-30T11:05:47.425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.ml.feature.Tokenizer\n",
      "class org.apache.spark.ml.feature.HashingTF\n",
      "class org.apache.spark.ml.classification.LogisticRegression\n",
      "class org.apache.spark.ml.Pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_f117a88d34eb\n",
       "hashingTF = hashingTF_83b157337b02\n",
       "lr = logreg_9a278b7ad216\n",
       "pipe = pipeline_bbe1e386268d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_bbe1e386268d"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "println(tokenizer.getClass)\n",
    "\n",
    "val hashingTF = new HashingTF().setNumFeatures(100)\n",
    "   .setInputCol(tokenizer.getOutputCol)\n",
    "   .setOutputCol(\"features\")\n",
    "println(hashingTF.getClass)\n",
    "\n",
    "val lr = new LogisticRegression().setLabelCol(\"label\")\n",
    "//println(lr.explainParams)\n",
    "lr.setFeaturesCol(\"features\").setMaxIter(10).setRegParam(0.01)\n",
    "println(lr.getClass)\n",
    "\n",
    "val pipe = new Pipeline().setStages(Array(tokenizer,hashingTF,lr))\n",
    "println(pipe.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3，训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:21:10.812706Z",
     "start_time": "2019-09-30T09:21:10.123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.ml.PipelineModel"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model = pipeline_bbe1e386268d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_bbe1e386268d"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipe.fit(dftrain)\n",
    "print(model.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:08:05.789848Z",
     "start_time": "2019-09-30T09:08:05.703Z"
    }
   },
   "source": [
    "**4，使用模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:27:35.556496Z",
     "start_time": "2019-09-30T09:27:34.887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  7|           spark job|  1.0|\n",
      "|  9|         hello world|  0.0|\n",
      "| 10|           a b c d e|  0.0|\n",
      "| 11|      you can you up|  0.0|\n",
      "| 12|spark is easy to ...|  1.0|\n",
      "+---+--------------------+-----+\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|                text|            features|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|           spark job|(100,[5,70],[1.0,...|[0.35046042897668...|       1.0|\n",
      "|         hello world|(100,[48,50],[1.0...|[0.33560921515516...|       1.0|\n",
      "|           a b c d e|(100,[22,61,70,78...|[0.19082246657270...|       1.0|\n",
      "|      you can you up|(100,[25,28,33],[...|[0.81519423235142...|       0.0|\n",
      "|spark is easy to ...|(100,[5,21,60,81,...|[0.47768327161195...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dftest = [id: bigint, text: string ... 1 more field]\n",
       "dfresult = [id: bigint, text: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, text: string ... 6 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dftest = Seq((7L,\"spark job\",1.0),(9L,\"hello world\",0.0),\n",
    "                 (10L,\"a b c d e\",0.0),(11L,\"you can you up\",0.0),\n",
    "                (12L,\"spark is easy to use.\",1.0)).toDF(\"id\",\"text\",\"label\")\n",
    "dftest.show\n",
    "\n",
    "val dfresult = model.transform(dftest)\n",
    "\n",
    "dfresult.selectExpr(\"text\",\"features\",\"probability\",\"prediction\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:08:10.813260Z",
     "start_time": "2019-09-30T09:08:10.532Z"
    }
   },
   "source": [
    "**5，评估模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfresult.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:36:42.815369Z",
     "start_time": "2019-09-30T09:36:42.376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name (default: label, current: label)\n",
      "metricName: metric name in evaluation (f1|weightedPrecision|weightedRecall|accuracy) (default: f1, current: f1)\n",
      "predictionCol: prediction column name (default: prediction, current: prediction)\n",
      "\n",
      "accuracy = 0.5666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_36c245695257\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_36c245695257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"f1\")\n",
    "    .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "\n",
    "println(evaluator.explainParams())\n",
    "\n",
    "evaluator.evaluate(dfresult)\n",
    "\n",
    "println(s\"\\naccuracy = ${evaluator.evaluate(dfresult)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6，保存模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:11:36.792176Z",
     "start_time": "2019-09-30T11:11:36.206Z"
    }
   },
   "outputs": [],
   "source": [
    "model.write.overwrite().save(\"mymodel.model\")\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "//model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// We can also save this unfit pipeline to disk\n",
    "//pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:52:05.433590Z",
     "start_time": "2019-09-30T09:52:04.533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|           spark job|  1.0|       1.0|\n",
      "|         hello world|  0.0|       1.0|\n",
      "|           a b c d e|  0.0|       1.0|\n",
      "|      you can you up|  0.0|       0.0|\n",
      "|spark is easy to ...|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_loaded = pipeline_bbe1e386268d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_bbe1e386268d"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//重新载入模型\n",
    "val model_loaded = PipelineModel.load(\"mymodel.model\")\n",
    "model_loaded.transform(dftest).select(\"text\",\"label\",\"prediction\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用spark.read导入csv，image，libsvm，txt等格式数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:52:35.459487Z",
     "start_time": "2019-09-30T11:52:35.114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfimage = [image: struct<origin: string, height: int ... 4 more fields>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[image: struct<origin: string, height: int ... 4 more fields>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//导入图片\n",
    "\n",
    "val dfimage = spark.read.format(\"image\").option(\"dropInvalid\", true).load(\"imagedata\")\n",
    "dfimage.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:52:46.987458Z",
     "start_time": "2019-09-30T11:52:40.230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "|              origin|height|width|nChannels|mode|                data|\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "|file:///Users/lia...|   640|  640|        3|  16|[04 13 33 02 11 3...|\n",
      "|file:///Users/lia...|   287|  562|        4|  24|[91 4D 2F FF C7 6...|\n",
      "|file:///Users/lia...|   276|  619|        3|  16|[E3 E0 BB E3 E0 B...|\n",
      "|file:///Users/lia...|   338|  600|        3|  16|[A7 7E 5E A8 7F 5...|\n",
      "+--------------------+------+-----+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfimage.selectExpr(\"image.*\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四，特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark的特征处理功能主要在 spark.ml.feature 模块中，包括以下一些功能。\n",
    "\n",
    "* 特征提取：Tf-idf, Word2Vec, CountVectorizer, FeatureHasher\n",
    "\n",
    "* 特征转换：OneHotEncoderEstimator, Normalizer, Imputer(缺失值填充), StandardScaler, MinMaxScaler, Tokenizer(构建词典), \n",
    "  StopWordsRemover, SQLTransformer, Bucketizer, Interaction(交叉项), Binarizer(二值化), n-gram,……\n",
    "\n",
    "* 特征选择：VectorSlicer(向量切片), RFormula, ChiSqSelector(卡方检验)\n",
    "\n",
    "* LSH转换：局部敏感哈希广泛用于海量数据中求最邻近，聚类等算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1，CountVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer可以提取文本中的词频特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------------+\n",
      "| id|          words|            features|\n",
      "+---+---------------+--------------------+\n",
      "|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|\n",
      "|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|\n",
      "+---+---------------+--------------------+\n",
      "\n",
      "+---+---------------+--------------------+\n",
      "| id|          words|            features|\n",
      "+---+---------------+--------------------+\n",
      "|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|\n",
      "|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|\n",
      "+---+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: int, words: array<string>]\n",
       "cvModel = cntVec_324b7869593e\n",
       "cvm = cntVecModel_4947f6142271\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cntVecModel_4947f6142271"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (0, Array(\"a\", \"b\", \"c\")),\n",
    "  (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\"))\n",
    ")).toDF(\"id\", \"words\")\n",
    "\n",
    "// fit a CountVectorizerModel from the corpus\n",
    "val cvModel: CountVectorizerModel = new CountVectorizer()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVocabSize(3)\n",
    "  .setMinDF(2)\n",
    "  .fit(df)\n",
    "\n",
    "// alternatively, define CountVectorizerModel with a-priori vocabulary\n",
    "val cvm = new CountVectorizerModel(Array(\"a\", \"b\", \"c\"))\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "cvModel.transform(df).show()\n",
    "\n",
    "cvm.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2，Tf-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-IDF可以降低文本频率过高的常用词的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentenceData = [label: double, sentence: string]\n",
       "tokenizer = tok_be783ff7cdf7\n",
       "wordsData = [label: double, sentence: string ... 1 more field]\n",
       "hashingTF = hashingTF_5689f46573df\n",
       "featurizedData = [label: double, sentence: string ... 2 more fields]\n",
       "idf = idf_2ce0d13b12f9\n",
       "idfModel = idf_2ce0d13b12f9\n",
       "rescaledData = [label: double, sentence: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, sentence: string ... 3 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "\n",
    "val sentenceData = spark.createDataFrame(Seq(\n",
    "  (0.0, \"Hi I heard about Spark\"),\n",
    "  (0.0, \"I wish Java could use case classes\"),\n",
    "  (1.0, \"Logistic regression models are neat\")\n",
    ")).toDF(\"label\", \"sentence\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "  .setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n",
    "\n",
    "val featurizedData = hashingTF.transform(wordsData)\n",
    "// alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)   \n",
    "\n",
    "val rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3，Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec可以使用浅层神经网络提取文本中词的相似语义信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.008142343163490296,0.02051363289356232,0.03255096450448036]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.043090314205203734,0.035048123182994974,0.023512658663094044]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.038572299480438235,-0.03250147425569594,-0.01552378609776497]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "documentDF = [text: array<string>]\n",
       "word2Vec = w2v_c890d08f1667\n",
       "model = w2v_c890d08f1667\n",
       "result = [text: array<string>, result: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[text: array<string>, result: vector]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Input data: Each row is a bag of words from a sentence or document.\n",
    "val documentDF = spark.createDataFrame(Seq(\n",
    "  \"Hi I heard about Spark\".split(\" \"),\n",
    "  \"I wish Java could use case classes\".split(\" \"),\n",
    "  \"Logistic regression models are neat\".split(\" \")\n",
    ").map(Tuple1.apply)).toDF(\"text\")\n",
    "\n",
    "// Learn a mapping from words to Vectors.\n",
    "val word2Vec = new Word2Vec()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"result\")\n",
    "  .setVectorSize(3)\n",
    "  .setMinCount(0)\n",
    "val model = word2Vec.fit(documentDF)\n",
    "\n",
    "val result = model.transform(documentDF)\n",
    "result.collect().foreach { case Row(text: Seq[_], features: Vector) =>\n",
    "  println(s\"Text: [${text.mkString(\", \")}] => \\nVector: $features\\n\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4， OnHotEncoderEstimator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoderEstimator可以将类别特征转换成OneHot编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [categoryIndex1: double, categoryIndex2: double]\n",
       "encoder = oneHotEncoder_1cc186061b9c\n",
       "model = oneHotEncoder_1cc186061b9c\n",
       "encoded = [categoryIndex1: double, categoryIndex2: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[categoryIndex1: double, categoryIndex2: double ... 2 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (0.0, 1.0),\n",
    "  (1.0, 0.0),\n",
    "  (2.0, 1.0),\n",
    "  (0.0, 2.0),\n",
    "  (0.0, 1.0),\n",
    "  (2.0, 0.0)\n",
    ")).toDF(\"categoryIndex1\", \"categoryIndex2\")\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"categoryIndex1\", \"categoryIndex2\"))\n",
    "  .setOutputCols(Array(\"categoryVec1\", \"categoryVec2\"))\n",
    "val model = encoder.fit(df)\n",
    "\n",
    "val encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5，FeatureHasher**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当特征数量过多时，可以用FeatureHasher来进行降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|real|bool |stringNum|string|features                                                |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
      "|3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
      "|4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
      "|5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset = [real: double, bool: boolean ... 2 more fields]\n",
       "hasher = featureHasher_e574faff1d9a\n",
       "featurized = [real: double, bool: boolean ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[real: double, bool: boolean ... 3 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.FeatureHasher\n",
    "\n",
    "val dataset = spark.createDataFrame(Seq(\n",
    "  (2.2, true, \"1\", \"foo\"),\n",
    "  (3.3, false, \"2\", \"bar\"),\n",
    "  (4.4, false, \"3\", \"baz\"),\n",
    "  (5.5, false, \"4\", \"foo\")\n",
    ")).toDF(\"real\", \"bool\", \"stringNum\", \"string\")\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    "  .setInputCols(\"real\", \"bool\", \"stringNum\", \"string\")\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val featurized = hasher.transform(dataset)\n",
    "featurized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6, IndexToString 和 StringIndexer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IndexToString和StringIndexer互为逆操作。\n",
    "\n",
    "前者可以将数字表示的类别特征转换成字符串表示的类别特征。\n",
    "\n",
    "后者可以将字符串表示的类别特征转换成数字表示的类别特征。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'category' to indexed column 'categoryIndex'\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "StringIndexer will store labels in output column metadata: {\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"categoryIndex\"}\n",
      "\n",
      "Transformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: int, category: string]\n",
       "indexer = strIdx_210232843fab\n",
       "indexed = [id: int, category: string ... 1 more field]\n",
       "inputColSchema = StructField(categoryIndex,DoubleType,false)\n",
       "converter = idxToStr_d7d6742718af\n",
       "converted = [id: int, category: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, category: string ... 2 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.attribute.Attribute\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer}\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (0, \"a\"),\n",
    "  (1, \"b\"),\n",
    "  (2, \"c\"),\n",
    "  (3, \"a\"),\n",
    "  (4, \"a\"),\n",
    "  (5, \"c\")\n",
    ")).toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"category\")\n",
    "  .setOutputCol(\"categoryIndex\")\n",
    "  .fit(df)\n",
    "val indexed = indexer.transform(df)\n",
    "\n",
    "println(s\"Transformed string column '${indexer.getInputCol}' \" +\n",
    "    s\"to indexed column '${indexer.getOutputCol}'\")\n",
    "indexed.show()\n",
    "\n",
    "val inputColSchema = indexed.schema(indexer.getOutputCol)\n",
    "println(s\"StringIndexer will store labels in output column metadata: \" +\n",
    "    s\"${Attribute.fromStructField(inputColSchema).toString}\\n\")\n",
    "\n",
    "val converter = new IndexToString()\n",
    "  .setInputCol(\"categoryIndex\")\n",
    "  .setOutputCol(\"originalCategory\")\n",
    "\n",
    "val converted = converter.transform(indexed)\n",
    "\n",
    "println(s\"Transformed indexed column '${converter.getInputCol}' back to original string \" +\n",
    "    s\"column '${converter.getOutputCol}' using labels in metadata\")\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"categoryIndex\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.schema(indexer.getOutputCol)\n",
    "\n",
    "Attribute.fromStructField(inputColSchema).toString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7，StandardScaler 正态标准化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataFrame = [label: double, features: vector]\n",
       "scaler = stdScal_62b0d9d05e04\n",
       "scalerModel = stdScal_62b0d9d05e04\n",
       "scaledData = [label: double, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val dataFrame = spark.read.format(\"libsvm\").load(\"/Users/liangyun/ProgramFiles/\"+\n",
    "        \"spark-2.4.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "\n",
    "// Compute summary statistics by fitting the StandardScaler.\n",
    "val scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8, MinMax标准化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.0, 1.0]\n",
      "+--------------+--------------+\n",
      "|      features|scaledFeatures|\n",
      "+--------------+--------------+\n",
      "|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|\n",
      "| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n",
      "|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataFrame = [id: int, features: vector]\n",
       "scaler = minMaxScal_79c98923f40c\n",
       "scalerModel = minMaxScal_79c98923f40c\n",
       "scaledData = [id: int, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.MinMaxScaler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val dataFrame = spark.createDataFrame(Seq(\n",
    "  (0, Vectors.dense(1.0, 0.1, -1.0)),\n",
    "  (1, Vectors.dense(2.0, 1.1, 1.0)),\n",
    "  (2, Vectors.dense(3.0, 10.1, 3.0))\n",
    ")).toDF(\"id\", \"features\")\n",
    "\n",
    "val scaler = new MinMaxScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "// Compute summary statistics and generate MinMaxScalerModel\n",
    "val scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "// rescale each feature to range [min, max].\n",
    "val scaledData = scalerModel.transform(dataFrame)\n",
    "println(s\"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]\")\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9，MaxAbsScaler标准化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|      features|  scaledFeatures|\n",
      "+--------------+----------------+\n",
      "|[1.0,0.1,-8.0]|[0.25,0.01,-1.0]|\n",
      "|[2.0,1.0,-4.0]|  [0.5,0.1,-0.5]|\n",
      "|[4.0,10.0,8.0]|   [1.0,1.0,1.0]|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataFrame = [id: int, features: vector]\n",
       "scaler = maxAbsScal_89f335f08a40\n",
       "scalerModel = maxAbsScal_89f335f08a40\n",
       "scaledData = [id: int, features: vector ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, features: vector ... 1 more field]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.MaxAbsScaler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val dataFrame = spark.createDataFrame(Seq(\n",
    "  (0, Vectors.dense(1.0, 0.1, -8.0)),\n",
    "  (1, Vectors.dense(2.0, 1.0, -4.0)),\n",
    "  (2, Vectors.dense(4.0, 10.0, 8.0))\n",
    ")).toDF(\"id\", \"features\")\n",
    "\n",
    "val scaler = new MaxAbsScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "// Compute summary statistics and generate MaxAbsScalerModel\n",
    "val scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "// rescale each feature to range [-1, 1]\n",
    "val scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10，SQLTransformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用SQL语法将DataFrame进行转换，等效于注册表的作用。\n",
    "\n",
    "但它可以用于Pipeline中作为Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: int, v1: double ... 1 more field]\n",
       "sqlTrans = sql_f7174c4e1104\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sql_f7174c4e1104"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.SQLTransformer\n",
    "\n",
    "val df = spark.createDataFrame(\n",
    "  Seq((0, 1.0, 3.0), (2, 2.0, 5.0))).toDF(\"id\", \"v1\", \"v2\")\n",
    "\n",
    "val sqlTrans = new SQLTransformer().setStatement(\n",
    "  \"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "\n",
    "sqlTrans.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11, Imputer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputer转换器可以填充缺失值，缺失值可以用 Double.NaN来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [a: double, b: double]\n",
       "imputer = imputer_85599015b326\n",
       "model = imputer_85599015b326\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "imputer_85599015b326"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (1.0, Double.NaN),\n",
    "  (2.0, Double.NaN),\n",
    "  (Double.NaN, 3.0),\n",
    "  (4.0, 4.0),\n",
    "  (5.0, 5.0)\n",
    ")).toDF(\"a\", \"b\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(Array(\"a\", \"b\"))\n",
    "  .setOutputCols(Array(\"out_a\", \"out_b\"))\n",
    "\n",
    "val model = imputer.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12 ChiSqSelector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当label是离散值时，ChiSqSelector选择器可以根据Chi2检验统计量筛选特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List((7,[0.0,0.0,18.0,1.0],1.0), (8,[0.0,1.0,12.0,0.0],0.0), (9,[1.0,0.0,15.0,0.1],0.0))\n",
       "df = [id: int, features: vector ... 1 more field]\n",
       "selector = chiSqSelector_6f2ed137f410\n",
       "result = [id: int, features: vector ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, features: vector ... 2 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val data = Seq(\n",
    "  (7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0),\n",
    "  (8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0),\n",
    "  (9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0)\n",
    ")\n",
    "\n",
    "val df = spark.createDataset(data).toDF(\"id\", \"features\", \"clicked\")\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(1)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"clicked\")\n",
    "  .setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "val result = selector.fit(df).transform(df)\n",
    "\n",
    "println(s\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13，局部敏感哈希（LSH）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locality Sensitive Hashing 是一种广泛使用的哈希技巧，常用于求海量数据的最邻近，聚类和离群值检测等任务中。\n",
    "\n",
    "局部敏感哈希的特性是将距离较近的点以很大的概率通过哈希作用映射到相同的桶内，而将距离较远的点以很大的概率通过哈希作用映射到不同的桶中。\n",
    "\n",
    "BucketedRandomProjectionLSH 使用Euclidean distance作为距离度量，取点在某个随机方向的投影结果作为哈希值。\n",
    "\n",
    "MinHashLSH 使用Jaccard distance作为距离度量，它是衡量两个集合差异性的一个指标，MinHashLSH取集合中所有元素的哈希值的最小值作为哈希值。\n",
    "\n",
    "杰拉德距离计算公式如下:$$(A,B)=1−\\frac{|A∩B|}{|A∪B|$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+-----------+--------------------+\n",
      "| id|   features|              hashes|\n",
      "+---+-----------+--------------------+\n",
      "|  0|  [1.0,1.0]|[[0.0], [0.0], [-...|\n",
      "|  1| [1.0,-1.0]|[[-1.0], [-1.0], ...|\n",
      "|  2|[-1.0,-1.0]|[[-1.0], [-1.0], ...|\n",
      "|  3| [-1.0,1.0]|[[0.0], [0.0], [-...|\n",
      "+---+-----------+--------------------+\n",
      "\n",
      "Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\n",
      "+---+---+-----------------+\n",
      "|idA|idB|EuclideanDistance|\n",
      "+---+---+-----------------+\n",
      "|  1|  4|              1.0|\n",
      "|  0|  6|              1.0|\n",
      "|  1|  7|              1.0|\n",
      "|  3|  5|              1.0|\n",
      "|  0|  4|              1.0|\n",
      "|  3|  6|              1.0|\n",
      "|  2|  7|              1.0|\n",
      "|  2|  5|              1.0|\n",
      "+---+---+-----------------+\n",
      "\n",
      "Approximately searching dfA for 2 nearest neighbors of the key:\n",
      "+---+----------+--------------------+-------+\n",
      "| id|  features|              hashes|distCol|\n",
      "+---+----------+--------------------+-------+\n",
      "|  0| [1.0,1.0]|[[0.0], [0.0], [-...|    1.0|\n",
      "|  1|[1.0,-1.0]|[[-1.0], [-1.0], ...|    1.0|\n",
      "+---+----------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfA = [id: int, features: vector]\n",
       "dfB = [id: int, features: vector]\n",
       "key = [1.0,0.0]\n",
       "brp = brp-lsh_dd546f0c3270\n",
       "model = brp-lsh_dd546f0c3270\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "brp-lsh_dd546f0c3270"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.BucketedRandomProjectionLSH\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val dfA = spark.createDataFrame(Seq(\n",
    "  (0, Vectors.dense(1.0, 1.0)),\n",
    "  (1, Vectors.dense(1.0, -1.0)),\n",
    "  (2, Vectors.dense(-1.0, -1.0)),\n",
    "  (3, Vectors.dense(-1.0, 1.0))\n",
    ")).toDF(\"id\", \"features\")\n",
    "\n",
    "val dfB = spark.createDataFrame(Seq(\n",
    "  (4, Vectors.dense(1.0, 0.0)),\n",
    "  (5, Vectors.dense(-1.0, 0.0)),\n",
    "  (6, Vectors.dense(0.0, 1.0)),\n",
    "  (7, Vectors.dense(0.0, -1.0))\n",
    ")).toDF(\"id\", \"features\")\n",
    "\n",
    "val key = Vectors.dense(1.0, 0.0)\n",
    "\n",
    "val brp = new BucketedRandomProjectionLSH()\n",
    "  .setBucketLength(2.0)\n",
    "  .setNumHashTables(3)\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"hashes\")\n",
    "\n",
    "val model = brp.fit(dfA)\n",
    "\n",
    "// Feature Transformation\n",
    "println(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n",
    "// Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "// similarity join.\n",
    "// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "// `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "println(\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, \"EuclideanDistance\")\n",
    "  .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "    col(\"datasetB.id\").alias(\"idB\"),\n",
    "    col(\"EuclideanDistance\")).show()\n",
    "\n",
    "// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "// neighbor search.\n",
    "// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "// `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "println(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+--------------------+--------------------+\n",
      "| id|            features|              hashes|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[2.25592966E8], ...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[2.25592966E8], ...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|[[2.25592966E8], ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n",
      "Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\n",
      "+---+---+---------------+\n",
      "|idA|idB|JaccardDistance|\n",
      "+---+---+---------------+\n",
      "|  1|  4|            0.5|\n",
      "|  0|  5|            0.5|\n",
      "|  1|  5|            0.5|\n",
      "|  2|  5|            0.5|\n",
      "+---+---+---------------+\n",
      "\n",
      "Approximately searching dfA for 2 nearest neighbors of the key:\n",
      "+---+--------------------+--------------------+-------+\n",
      "| id|            features|              hashes|distCol|\n",
      "+---+--------------------+--------------------+-------+\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[2.25592966E8], ...|   0.75|\n",
      "+---+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfA = [id: int, features: vector]\n",
       "dfB = [id: int, features: vector]\n",
       "key = (6,[1,3],[1.0,1.0])\n",
       "mh = mh-lsh_ce80a4f003d3\n",
       "model = mh-lsh_ce80a4f003d3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mh-lsh_ce80a4f003d3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.MinHashLSH\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val dfA = spark.createDataFrame(Seq(\n",
    "  (0, Vectors.sparse(6, Seq((0, 1.0), (1, 1.0), (2, 1.0)))),\n",
    "  (1, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (4, 1.0)))),\n",
    "  (2, Vectors.sparse(6, Seq((0, 1.0), (2, 1.0), (4, 1.0))))\n",
    ")).toDF(\"id\", \"features\")\n",
    "\n",
    "val dfB = spark.createDataFrame(Seq(\n",
    "  (3, Vectors.sparse(6, Seq((1, 1.0), (3, 1.0), (5, 1.0)))),\n",
    "  (4, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (5, 1.0)))),\n",
    "  (5, Vectors.sparse(6, Seq((1, 1.0), (2, 1.0), (4, 1.0))))\n",
    ")).toDF(\"id\", \"features\")\n",
    "\n",
    "val key = Vectors.sparse(6, Seq((1, 1.0), (3, 1.0)))\n",
    "\n",
    "val mh = new MinHashLSH()\n",
    "  .setNumHashTables(5)\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"hashes\")\n",
    "\n",
    "val model = mh.fit(dfA)\n",
    "\n",
    "// Feature Transformation\n",
    "println(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n",
    "// Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "// similarity join.\n",
    "// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "// `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`\n",
    "println(\"Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 0.6, \"JaccardDistance\")\n",
    "  .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "    col(\"datasetB.id\").alias(\"idB\"),\n",
    "    col(\"JaccardDistance\")).show()\n",
    "\n",
    "// Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "// neighbor search.\n",
    "// We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "// `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "// It may return less than 2 rows when not enough approximate near-neighbor candidates are\n",
    "// found.\n",
    "println(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五，分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六，回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 七，聚类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 八，降维模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 九，模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 十，统计工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
